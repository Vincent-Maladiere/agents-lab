{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# RAG Agent\n\nThis notebook takes inspiration from Pydantic-AI [RAG tutorial](https://ai.pydantic.dev/examples/rag/)\nand simplifies its approach.\n\n## Context\n\nOur objective is to create an assistant that answers questions about Logfire\n(developed by Pydantic) using the project's documentation as a knowledge base. \nWe make this RAG agentic by providing a `retrieve` function as a tool for the LLM.\n\n.. mermaid::\n\n   flowchart TD\n      A(User) -->|Ask a question| B(Agent)\n      B --> |Messages + retrieve tool|C{LLM}\n      C --> |Text or Tool Calling response|B\n      B --> |Vectorize user query |D(Knowledge base)\n      D --> |Return documents closest to query|B\n      B --> |Return answer|A\n\n\nIn this system, the LLM has access to the \"retrieve\" tool, which it may or may not\ninvoke in its response. If invoked, the tool call is parsed by the LLM client\nand returned as a structured response to the agent, which executes the requested\nfunction.\n\nThis differs from **workflow-based RAG**, where the retrieved function is always\nexecuted before calling the LLM. In workflow-based RAG, the LLM prompt is a\nconcatenation of the initial prompt and the retrieved content.\nFor more detailed information, we recommend exploring\n[Ragger-Duck](https://probabl-ai.github.io/sklearn-ragger-duck/user_guide/index.html),\na RAG implementation developed by scikit-learn core developers.\n\n.. figure:: ../_static/rag_workflow.png\n\n   A RAG workflow (source: Ragger-Duck)\n\n   \n### RAG Workflow vs Agent-Based RAG\n\nWe revisit the \"workflow vs agent-based systems\" tradeoff mentioned in the previous\nnotebook. This assistant use-case requires a high degree of flexibility, as user\nqueries are arbitrary. To reduce costs and latency, we only query the knowledge base\nwhen necessary based on user input.\n   \n## Implementation\n\n### Design choices\n\nWe simplify the original pydantic tutorial with the following optimizations:\n\n- **No vector database**:\n\n  - For small knowledge bases, overall latency can be reduced by keeping the data\n    in memory using a dataframe or a numpy array, persisted with [diskcache](https://grantjenks.com/docs/diskcache/).\n  - Approximate nearest neighbors (ANN) operations, typically provided by a vector\n    database, can be replaced by a simple Nearest Neighbors estimator from scikit-learn.\n\n- **Batch vectorization**:\n  \n  - The content of the knowledge base is vectorized in a single batch rather\n    than looping through each element individually.\n\n- **Local vectorization**:\n\n  - To reduce API costs, we vectorize content locally by downloading a text vectorizer\n    from HuggingFace. This is achieved using ``skrub``, which wraps the\n    ``sentence-transformers`` library to provide a scikit-learn-compatible transformer.\n    No GPU is required.\n\nOverall, this approach is faster to execute while remaining scalable\nfor reasonably sized knowledge bases, making it more efficient than the original\nPydantic tutorial.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Building the knowledge base\n\nWe begin by fetching the Logfire documentation archived online. Next, we use skrub's\ndataframe visualization to display long text more efficiently (click on any cell\nto view its text content).\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import pandas as pd\nimport requests\nfrom skrub import patch_display\n\n\n# Replace pandas' dataframe visualisation with skrub's\npatch_display()\n\nDOCS_JSON = (\n    'https://gist.githubusercontent.com/'\n    'samuelcolvin/4b5bb9bb163b1122ff17e29e48c10992/raw/'\n    '80c5925c42f1442c24963aaf5eb1a324d47afe95/logfire_docs.json'\n)\n\ndoc_pages = pd.DataFrame(\n    requests.get(DOCS_JSON).json()\n)\ndoc_pages"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We now use a text embedding model to get a vectorized representation of each page\nof our knowledge base.\nWe can choose among various type of vectorizer:\n\n.. list-table::\n   :header-rows: 1\n\n   * - Type\n     - Example\n     - Advantages\n     - Caveats \n   * - Ngram-based\n     - BM25, TF-IDF, LSA, MinHash\n     - Fast and cheap to train. Good baselines.\n     - Lack flexibility, corpus dependent.\n   * - Pre-trained text encoder, open weight\n     - BERT, e5-v2, any model on sentence-transformers\n     - More powerful embedding representations, local inference.\n     - Requires installing pytorch and extra dependencies.\n   * - Pre-trained text encoder, commercial API\n     - open-ai text-embedding-3-small\n     - Most powerful representations, using techniques like\n       [Matryoshka representation learning](https://arxiv.org/abs/2205.13147)\n       (also available on sentence-transformers). Easy API integration.\n     - Inference costs, reliance on a third party, closed weights, batch size < 2048.\n\nFor this example, we choose the second option, as it reduces inference cost.\nskrub's TextEncoder downloads the specified model locally using sentence-transformers,\ntransformers and pytorch before generating the embeddings for the knowledge base.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from skrub import TextEncoder\n\n\ntext_encoder = TextEncoder(\n    model_name=\"sentence-transformers/paraphrase-albert-small-v2\",\n    n_components=None,\n)\nembeddings = text_encoder.fit_transform(doc_pages[\"content\"])\nembeddings.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next, we use scikit-learn ``NearestNeighbors`` to perform exact retrieval. Note that\nthis operation has a time complexity of $O(d \\times N)$, where $d$ is\nthe dimensionality of our embedding vectors, and $N$ the number of elements\nto scan. For larger knowledge bases, using Approximate Nearest Neighbors, with\ntechniques like HNSW (implemented by [faiss](https://faiss.ai/)) or random\nprojections (implemented by [Annoy](https://github.com/spotify/annoy))\nis recommended, as these reduce the retrieval time complexity to\n$O(d \\times log(N))$.\n\nWe return the indices of the 8 closest match and their distances for two queries:\none related query to our knowledge base topic (Logfire) and another one unrelated\n(cooking).\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from sklearn.neighbors import NearestNeighbors\n\n\nnn = NearestNeighbors(n_neighbors=8).fit(embeddings)\n\nquery_embedding = text_encoder.transform(\n    pd.Series([\n        \"How do I configure logfire to work with FastAPI?\",\n        \"I'm a chef, explain how to bake Lasagnas.\",\n    ])\n)\n\ndistances, indices = nn.kneighbors(query_embedding, return_distance=True)\n\nprint(distances[0])\ndoc_pages.iloc[indices[0]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We observe that we successfully retrieved content related to FastAPI in the\ndocumentation. What are the results for the unrelated query?\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(distances[1])\ndoc_pages.iloc[indices[1]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For the second query, we can hardly discern a link between the retrieved items and\nthe original question. However, notice that their distances are higher compared\nto the first query. This means that no article closely match the second query.\n\nThe average distances between the first and second queries are quite similar, though.\nThis issue is commonly referred as the **curse of dimensionality**, where items in\nhigh-dimensional spaces tends to all appear \"far\" from each other due to\nthe hyper-volume growing exponentially with the number of dimensions. Real-world\nimplementations require a careful evaluation of retrieval system performance, which\nwe skip here.\n\nA possible filtering method would be to set a radius, i.e., a maximum distance beyond\nwhich retrieved elements are discarded. As shown below, the second query results in\nan empty set, as all euclidean distances exceed 14.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "nn.radius_neighbors(\n    query_embedding,\n    radius=14,\n    return_distance=True,\n    sort_results=True,\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can emulate persistence on disk using diskcache. Originally designed as a\nfast key-value storage solution for Django, it can also be applied in our\nagentic context. Here, we serialize the knowledge base content, our text encoder,\nand the fitted nearest neighbors estimator.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import diskcache as dc\n\n\ncache = dc.Cache('tmp')\ncache[\"doc_pages\"] = doc_pages\ncache[\"text_encoder\"] = text_encoder\ncache[\"nn\"] = nn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Defining the Agent\n\nWe defined our pydantic-ai Agent with its retrieve function set as a tool.\nNotice how pydantic-ai enables you to specify a schema for the dependency ``Deps``,\nwhich is used as a ``RunContext`` during tool execution.\n\nFor this example, we use OpenAI GPT-4o-mini rather than Llama3.3-70B with Groq's free\ntier as Groq currently struggles with tool calling.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import logfire\nfrom dotenv import load_dotenv\nfrom dataclasses import dataclass\nfrom pydantic_ai import RunContext\nfrom pydantic_ai.agent import Agent\nimport nest_asyncio\n\n\n# Load the 'OPENAI_API_KEY' variable environment from a source file.\nload_dotenv()\n\n# Enable nested event loop in jupyter notebooks to run pydantic-ai\n# asynchronous coroutines.\nnest_asyncio.apply()\n\n# Some boilerplate around logging.\nlogfire.configure(scrubbing=False)\n\n@dataclass\nclass Deps:\n    text_encoder: TextEncoder\n    nn: NearestNeighbors\n    doc_pages: pd.DataFrame\n\nsystem_prompt = (\n    \"You are a documentation assistant. Your objective is to answer user questions \"\n    \"by retrieving the right articles in the documentation. \"\n    \"Don't look-up the documentation if the question is unrelated to LogFire \"\n    \"or Pydantic. \"\n)\n\nagent = Agent(\n    'openai:gpt-4o-mini',\n    system_prompt=system_prompt,\n    deps_type=Deps,\n)\n\ndef make_prompt(pages):\n    return \"\\n\\n\".join(\n        (\n            \"# \" + pages[\"title\"]\n            + \"\\nDocumentation URL:\" + pages[\"path\"]\n            + \"\\n\\n\" + pages[\"content\"] + \"\\n\"\n        ).tolist()\n    )\n\n@agent.tool\nasync def retrieve(context: RunContext[Deps], search_query: str) -> str:\n    \"\"\"Retrieve documentation sections based on a search query.\n\n    Args:\n        context: The call context.\n        search_query: The search query.\n    \"\"\"\n    with logfire.span(f'create embedding for {search_query=}'):\n        query_embedding = context.deps.text_encoder.transform(\n            pd.Series([search_query]),\n        )\n\n    indices = (\n        context.deps.nn.kneighbors(query_embedding, return_distance=False)\n        .squeeze()\n    )\n\n    pages = context.deps.doc_pages.iloc[indices]\n    doc_retrieved = make_prompt(pages)\n\n    print(doc_retrieved)\n\n    return doc_retrieved"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, we define our main coroutine entry point to run the agent.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "async def run_agent(question: str):\n    \"\"\"\n    Entry point to run the agent and perform RAG based question answering.\n    \"\"\"\n    logfire.info(f'Asking \"{question}\"')\n\n    cache = dc.Cache('tmp')\n\n    deps = Deps(\n        text_encoder=cache[\"text_encoder\"],\n        nn=cache[\"nn\"],\n        doc_pages=cache[\"doc_pages\"],\n    )\n    return await agent.run(question, deps=deps)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Results\nWe are now ready run our system! Logfire will generate logs for the different steps\nto help us observe the different internal steps.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import asyncio\n\n\nanswer = asyncio.run(\n    run_agent(\"Can you summarize the roadmap for Logfire?\")\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's now display the final response:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(answer.data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The display below shows the sequence of messages from top to bottom (most recent).\n\n1. The LLM correctly responded to our first query by calling a retrieval tool.\n2. After retrieving the content queried by the LLM, we make another call with\n   this content.\n3. Finally, the LLM sends back its text response, completing the message loop.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import json\nfrom pprint import pprint\n\n\npprint(\n    json.loads(answer.all_messages_json())\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's now observe the agent behavior for an unrelated query.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "unrelated_answer = asyncio.run(\n    run_agent(\"I'm a chef, explain how to bake a delicious brownie?\")\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Since we specified in the agent system prompt not to perform a retrieval operation\nfor unrelated questions, the agent responds with a plain text message indicating\nits inability to answer.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(unrelated_answer.data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As expected, the message loop is smaller since the LLM didn't invoke the retrieve\nfunction, resulting in less latency and lower inference costs.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "pprint(\n    json.loads(unrelated_answer.all_messages_json())\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, we cleanup the ``tmp`` diskcache folder.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import shutil\n\n\nshutil.rmtree('tmp')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}