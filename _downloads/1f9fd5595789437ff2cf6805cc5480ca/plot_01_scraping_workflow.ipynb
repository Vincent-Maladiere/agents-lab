{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Web Scraping Workflow\n\nIn this notebook, we explore how [Pydantic-AI](https://ai.pydantic.dev/) workflows\ncan help us extract structured information from HTML pages. This example can be run\nentirely for free, and only requires signing up for\n[Groq's free tier](https://groq.com/).\n\n## Context\n\nScraping content from HTML pages has become ubiquitous, with use-cases ranging from\ngrowth marketing to dynamic pricing. A well-known challenge with web scraping is\nthe polymorphic and ever-changing HTML structure of websites, making scraper automation\ndifficult to maintain and scale. Developers often harcode the HTML paths of\nelements to be fetched, along with the logic for fetching them, resulting in brittle\nsolutions.\n\nResearch in LLMs offers promising avenues for performing web scraping more efficiently\ndue to the following capabilities:\n\n- longer context window, which can now contains large HTML DOMs\n  ([up to 128k tokens with llama 3](https://huggingface.co/blog/llama31#whats-new-with-llama-31))\n- structured output, adhering to predefined JSON schemas\n- better reasoning to interpret raw text and extract higher-level information\n- faster and cheaper inference, making LLM use economically viable\n\n## Strategy\n\n### Workflows vs agents\n\nIn most cases, web scraping does not require user-feedback, web search or tool\ninvocation. Instead, it typically consists of sequential, acyclic steps.\nThis makes simple workflows preferable for robustness and predictability\ncompared to agent-based systems. For an in-depth discussion of the trade-offs between\nworkflows and agents, refer to Anthropic's blogpost on\n[Building effective agents](https://www.anthropic.com/research/building-effective-agents).\n\n### HTML DOMs vs Screenshots\n\nRecent work such as\n[WebVoyager (He et al. 2024)](https://arxiv.org/abs/2401.13919), has explored \nusing screenshots to perform online actions (e.g. booking a flight) with agent-based\nsystem. For a practical implementation, see this\n[Langchain tutorial](https://langchain-ai.github.io/langgraph/tutorials/web-navigation/web_voyager).\n\nCurrently, the main limitations of the screenshot-based approach include:\n\n- Low average accuracy (~60%), due to the varying complexity of the websites and the\n  number of steps required to perform the task.\n- Limited choice of visual LLM. Since high definition screenshots are needed to read\n  text, only GPT4-V and GPT4-o are adapted to perform these benchmarks.\n- Limited use of textual information and HTML DOMs, screenshots rely heavily on visual\n  data, while textual information and HTML DOMs remain LLMs' primary mode of operation.\n\nOur use-case is simpler than WebVoyager, as it does not require performing actions or\nnavigating accross multiple websites. Instead, we deal with a few web pages processed\nsequentially.\n\nGiven this, our focus is on extracting HTML DOMs, while stripping away non-informative\ncontent such as styles or scripts. Beyond this automatic stripping, we avoid additional\nHTML transformation or lookups to keep the workflow as general and maintainable\nas possible.\n\n### Workflow\n\nOur use-case involves fetching information about car dealerships from a popular French\ne-commerce platform called \"LeBonCoin\" (LBC). To keep this notebook concise, we begin\nwith a list of dealership URLs, which were previously obtained using another scraping\nsystem.\n\nThe objective is to extract information from each dealerships' LBC page and enrich it\nwith financial data sourced from another website, \"pappers.fr\".\n\nOur workflow is the following:\n\n.. mermaid::\n\n   flowchart TD\n      A(Webdriver) -->|Browse LBC company url| B(LBC Agent)\n      B --> |Extract company info|C{Success}\n      C --> |Yes|D[Webdriver]\n      C -->|No| E[End]\n      D --> |Browse Pappers listing using 'company name'|F[Pappers Agent]\n      F --> |Find the company page from a list of companies, using name and city|G{Success}\n      G --> |Yes|H[Pappers Agent]\n      G --> |No|I[End]\n      H --> |Extract company financial info|J[Finish]\n\n\nOur webdriver uses a mix of ``requests`` for static pages (on LBC) and ``selenium`` where\nJavascript need to be enabled to access pages (on Pappers).\n\nWe define our HTML fetching functions below:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Implementation\n\n### LBC\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import time\nfrom bs4 import BeautifulSoup\nfrom dataclasses import dataclass\nfrom selenium import webdriver\nimport chromedriver_autoinstaller\n\n\nchromedriver_autoinstaller.install() \n\n# We monkey-patch `requests.get` because GitHub CI triggers LBC bot detection.\n@dataclass\nclass Response:\n    text: str\n    status_code: int = 200\n\n\ndef request_get_lbc(url, headers=None):\n    \"\"\"\n    Monkey-patch: return a static text file instead of making HTTP call.\n    \"\"\"\n    with open(\"../doc/_static/lbc_HTML_DOM.txt\") as f:\n        return Response(text=f.read())\n\n\ndef fetch_html_content(url, static_page=True, text_only=False):\n    \"\"\"Get the HTML DOM content of a URL.\n\n    Parameters\n    ----------\n    url : str\n        The url to be accessed\n    \n    static_page : bool, default=True\n        The strategy to fetch the content of a page.\n        - If True, the target page is considered static, and `requests.get` is used.\n        - If False, the target page requires Javascript and `selenium` is used\n          instead.\n    \n    text_only : bool, default=False\n        Whether or not to remove all HTML tags from the content.\n\n    Returns\n    -------\n    soup : BeautifulSoup\n        The content of the page parsed with bs4.\n    \"\"\"\n    # Headers to mimic a browser request\n    user_agent = (\n        \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 \"\n        \"(KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n    )\n    if static_page:\n        soup = _fetch_requests(url, user_agent)\n    else:\n        soup = _fetch_selenium(url, user_agent)\n    \n    # Strip all tags, and only return the text content of the page.\n    if text_only:\n        return soup.get_text(separator=\" \").strip()\n\n    return soup\n\n\ndef _fetch_requests(url, user_agent):\n    headers = {\"User-Agent\": user_agent}\n    response = request_get_lbc(url, headers=headers)\n\n    # Check if the request was successful\n    if response.status_code == 200:\n        # Parse the HTML content\n        return _parse_html(response.text)    \n    else:\n        raise ValueError(\n            f\"Failed to fetch the URL. Status code: {response.status_code}\"\n        )\n       \n\ndef _fetch_selenium(url, user_agent):\n    chrome_options = webdriver.ChromeOptions()\n    options = [\n        f\"--user-agent={user_agent}\",\n        \"--headless\",\n        \"--disable-gpu\",\n        \"--window-size=1920,1200\",\n        \"--ignore-certificate-errors\",\n        \"--disable-extensions\",\n        \"--no-sandbox\",\n        \"--disable-dev-shm-usage\",\n    ]\n    for option in options:\n        chrome_options.add_argument(option)\n\n    driver = webdriver.Chrome(options=chrome_options)\n    driver.get(url)\n\n    # Necessary to give the page time to load.\n    time.sleep(3)\n    \n    return _parse_html(driver.page_source)\n\n\ndef _parse_html(html):\n    soup = BeautifulSoup(html, \"html.parser\")\n\n    # Remove the following tags\n    for tag in soup([\"style\", \"script\", \"svg\", \"header\", \"head\"]):\n        tag.decompose()\n\n    # Remove the following attributes within tags\n    for tag in soup():\n        for attribute in [\"class\", \"id\", \"name\", \"style\"]:\n            del tag[attribute]\n\n    return soup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To keep this notebook concise and fast to execute, we will use only a single URL.\nSince the HTML structure is not required for this step, we will extract and retain \nonly the text content from the dealership page.\n\nYou can open the URL provided to review the outputs generated by the LLM, \nwhich will be produced in the next cell.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def print_with_token_count(text):\n    print(f\"character count: {len(text):,}\\n\")\n    print(text)\n\n\nurl_lbc = \"https://www.leboncoin.fr/boutique/17050/garage_hamelin.htm\"\ntext_content_lbc = fetch_html_content(url_lbc, static_page=True, text_only=True)\nprint_with_token_count(text_content_lbc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next, we pass the raw text content to a LLM. For this example, we choose the\nfollowing:\n\n- Groq: Used as our LLM endpoint, as it provides free access to the llama-3.3-70 model.\n- Pydantic-AI: Selected as our LLM client/framework due to its streamlined approach \n  to structuring responses, requiring less boilerplate compared to alternatives\n  like LangChain.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from pprint import pprint\nimport nest_asyncio\nfrom dotenv import load_dotenv\nfrom pydantic import BaseModel\nfrom pydantic_ai import Agent\n\n\n# Enable nested event loop in a notebook, so that we can run asynchrone coroutines in\n# pydantic-ai.\nnest_asyncio.apply()\n\n# Load GROQ_API_KEY from a source file placed in root.\nload_dotenv()\n\n# Our desired structured output.\nclass CompanyInfoLBC(BaseModel):\n    company_name: str\n    description: str\n    services_provided: str\n    number_of_cars: int\n    main_phone_number: str\n    country: str\n    city: str\n    full_address: str\n\nmodel_name = \"groq:llama-3.3-70b-versatile\"\nscraper_system_prompt = \"\"\"\nYou are a scrapping assistant, and your goal is to extract company information\nfrom html text provided by the user.\n\"\"\"\n\nagent_lbc = Agent(\n    model_name,\n    system_prompt=scraper_system_prompt,\n    result_type=CompanyInfoLBC,\n)\nresult_lbc = agent_lbc.run_sync(user_prompt=text_content_lbc)\ncompany_info = result_lbc.data.model_dump()\npprint(company_info)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We see that all fields are extracted as desired! Let's also observe the messaging\nsequence of Pydantic-AI:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import json\n\n\npprint(\n    json.loads(result_lbc.all_messages_json())\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Interestingly, we observe that the framework produced three messages (from top to\nbottom), but when looking at the Groq dev console, we notice that only a single API\ncall was made to the LLM.\n\nHere is the Pydantic-AI workflow:\n\n1. The first message is the request to the model, consisting of two parts: the system\n   prompt and the user prompt, which in this case is the HTML text.\n   Under the hood, \n   [pydantic-ai adds a structured output tool to the Groq client](https://github.com/pydantic/pydantic-ai/blob/16325844995f18977174638e9c4effc51036704e/pydantic_ai_slim/pydantic_ai/models/groq.py#L125-L132).\n2. Using this tool, Groq returns a JSON object, which pydantic-ai parses into a\n   Pydantic model.\n3. Finally, since the LLM indicates completion in step 2, pydantic-ai generates\n   a closing message and returns the result.\n\nTo quench our curiosity, here is the structured result tool passed to Groq:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from pprint import pprint\n\n\npprint(agent_lbc._result_schema.tool_defs())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Pappers\n\nThe next step in our workflow is to enrich the information from LBC with financial\ndata sourced from Pappers. This involves two LLM calls:\n\n1. Generate a Pappers search URL using the company name and access the resulting page.\n   This leads to a list of companies with similar names. We ask the LLM to identify\n   the company that best matches our query, based on the provided name and city.\n2. Generate a Pappers company URL to access the company's specific page, and then\n   prompt the LLM to extract the desired financial information.\n\nTo illustrate the first step, here is an example of how the company list appears:\n\n<img src=\"file://../_static/pappers_list.png\">\n\nNotice that the company we are searching for \u2013 located in Perpignan \u2013 is the third\nentry on the list!\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from urllib.parse import urljoin\n\nPAPPERS_BASE_URL = \"https://www.pappers.fr/\"\n\n\ndef make_pappers_search_url(company_name):\n    query = \"+\".join(company_name.split())\n    return urljoin(PAPPERS_BASE_URL, f\"recherche?q={query}\")\n\n\ndef make_pappers_company_url(company_href):\n    return urljoin(PAPPERS_BASE_URL, company_href)\n\n\npappers_search_url = make_pappers_search_url(company_info[\"company_name\"])\nprint(pappers_search_url)\n\nsoup = fetch_html_content(pappers_search_url, static_page=False, text_only=False)\nprint_with_token_count(str(soup))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We retain the HTML tags because they allow us to fetch the href corresponding\nto the company we are looking for. Keeping the HTML list structure helps the LLM\ndistinguish between the different items more effectively.\n\nThe downside is that the user prompt becomes quite large, exceeding 15,000 characters.\nThis increases inference costs and requires using an LLM with a large context window.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class CompanyHref(BaseModel):\n    href: str\n\nsystem_prompt_href = \"\"\"\n    You are a web scraping assistant. Your task is to find the item in a HTML list\n    which matches best the query company information. Only returns the href matching\n    this item.\n\"\"\"\n\nagent_pappers_href = Agent(\n    model_name,\n    system_prompt=system_prompt_href,\n    result_type=CompanyHref,\n)\n\nquery = {k: company_info[k] for k in [\"city\", \"company_name\"]}\nuser_prompt = f\"\"\"\n    query: <{query}>\n\n    html: <{soup}>\n\"\"\"\nresult_href = agent_pappers_href.run_sync(user_prompt)\nresult_href.data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This href corresponds to the Perpignan dealership we are searching for!\nNext, we complete the workflow by fetching financial information from the \ncompany's Pappers page.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "pappers_company_url = make_pappers_company_url(result_href.data.href)\nprint(pappers_company_url)\n\npappers_text_content = fetch_html_content(\n    pappers_company_url, static_page=False, text_only=True\n)\n\nclass FinancialInfo(BaseModel):\n    owner_name: str\n    owner_age: str\n    turnover: int\n    social_capital: int\n\nagent_pappers_info = Agent(\n    model_name,\n    system_prompt=scraper_system_prompt,\n    result_type=FinancialInfo,\n)\n\nuser_prompt = f\"html: <{pappers_text_content}>\"\npappers_info = agent_pappers_info.run_sync(user_prompt)\nfinancial_info = pappers_info.data.model_dump()\npprint(financial_info)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, we store the output in a database and synchronize the lead with our CRM.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "company_info.update(financial_info)\npprint(company_info)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}